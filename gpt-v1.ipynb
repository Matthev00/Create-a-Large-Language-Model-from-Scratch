{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple data Wizzard of Oz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\ufeff']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as file_handle:\n",
    "    text = file_handle.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18549/18549 [43:51<00:00,  7.05it/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2061/2061 [04:31<00:00,  7.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lzma\n",
    "from tqdm import tqdm\n",
    "\n",
    "def xz_files_in_dir(directory):\n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".xz\") and os.path.isfile(os.path.join(directory, filename)):\n",
    "            files.append(filename)\n",
    "    return files\n",
    "\n",
    "folder_path = \"E:/projekty python/Create-a-Large-Language-Model-from-Scratch/data/openwebtext\"\n",
    "output_file_train = \"train_split.txt\"\n",
    "output_file_val = \"val_split.txt\"\n",
    "vocab_file = \"vocab.txt\"\n",
    "\n",
    "files = xz_files_in_dir(folder_path)\n",
    "total_files = len(files)\n",
    "\n",
    "# Calculate the split indices\n",
    "split_index = int(total_files * 0.9) # 90% for training\n",
    "files_train = files[:split_index]\n",
    "files_val = files[split_index:]\n",
    "\n",
    "# Process the files for training and validation separately\n",
    "vocab = set()\n",
    "\n",
    "# Process the training files\n",
    "with open(output_file_train, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for filename in tqdm(files_train, total=len(files_train)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            characters = set(text)\n",
    "            vocab.update(characters)\n",
    "\n",
    "# Process the validation files\n",
    "with open(output_file_val, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for filename in tqdm(files_val, total=len(files_val)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            characters = set(text)\n",
    "            vocab.update(characters)\n",
    "\n",
    "# Write the vocabulary to vocab.txt\n",
    "with open(vocab_file, \"w\", encoding=\"utf-8\") as vfile:\n",
    "    for char in vocab:\n",
    "        vfile.write(char + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "max_iters = 3000\n",
    "learning_rate = 3e-4\n",
    "eval_steps = 500\n",
    "n_embd = 384\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32172\n"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open(\"vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "str_to_int = { ch:i for i, ch in enumerate(chars)}\n",
    "int_to_ch = { i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [str_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_ch[n] for n in l])\n",
    "\n",
    "encoded_hello = encode('hello')\n",
    "decoded_hello = decode(encoded_hello)\n",
    "print(decoded_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else \"val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "\n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHeadAttention class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" Simple linear layer followed by non_linear layer\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main class - The GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000èªžé ãˆ¡ð˜¼è·¥æ®¨ß¯å¸»à·šá¦æŽÆ¼å€¡å‡¤ð¬­ð“¼ç­ð¥¤«çº±å€Äºâ¦˜ä¸Žé—”ðŸ±æ®¯ðŸ‡¨ð˜°ðœ‚ð›€’â›½ð’¾ê™¬ä¾ášŠÆ—âˆð“…¤æ·šðŸ›íŸå¥å”€á•æ‹éº¥Ì–á¢“á¹˜æ³¿è‘›á”´é€¯Ì¤ðŸ‡¸ðŸ¢­æ¿®è“±â¢«ãƒ¥Ñ¹åš¢è©¹×“ðž¸«ê’‡êœ±å¼‚çŠ¹ï•·ï²ê­£ðž¹¼ð’‹‘æ„€å°‰á˜ð’¼Í»ï³¸ðŸ¦…ð’‹»í…ð£“à¥íœ˜à¾²é¢ì‰¿ðŸ“›ï¶§çž³ï˜½í›™æˆ°ðŸªâ˜˜ðŒ•â’¤â„¨ì¼“â¯€è³„ð“¡áŸ™ðŸ­æ®—ð“ƒ”êŠºî ´ð›¸â¸«ð¬‹âššê„¯æ¿¸Ì¸à«‰ð›‚¶ê²”ä¹–á¡€ð“‡–êŒ°æ¶²æ†Ú¯Å¿â¶€á”šé½•èŠç•”â• áŠ™ðŸ›¹ðŸ¥‰ì¶˜á²§æ–ŸíŸå¤©äº­âœ‹æžð™ å‚•ç´”ê‹á½«ðŸ‘›ç­·Ó‰éà¦œðŸ™–æ‰¿ã†¶æˆ²á†—è¿œè±ºã‰‰è¾²á¬³â²–ï¯£ó¾Ÿ³ðŸ§§ç›‚ê„†à¸”âºÔœÙ±à¥ªå¤è‹ç„‰ð›€‡â¼€å§—æ²“á¾·Ô‹á™ƒðˆâœ‡ðŸ¡¸å²¸è‘‘å‰‰âž‰ë†€â·£ê‹¥æš•ðŸ¤§áŽ ï»‹ð“ Ö›á…‹åš”çŸ¤â³‹ï¨™é‹ë“£ð’„†å’¦î¥¨â–¹á·“Ð°Ðžà´™ç²¹ï³Ÿð“¬é§”å´´á—ºî€¡ð¡”æ±¶á›¤Ø›å»¸ãŠºå¸§ç”¢á˜œã„Œâ•¼ãƒ­Í™ðŸ¡ë”ð’‡„ë°ê‰ˆáŸê‚šð“…†ð’Š¥â‹Ÿê®ƒë³»ï±ªà»•ä»½è½¢ð¦á™™ðŸ¢…ï¹¢ßºÚåŒ¹à¡ê¡¡à¹ã‚¶èŒé±“ë‹·á —äº‘á¹ð“‹«ð’“è½¤ðŒ¢â…¢î€­á†ŸðŽ¹âœŸï„¢ð“€è½¤ê›á€¶è´¬ïæª§æ»ŒÝƒëœ°æ¯îª‘ðŸž³ê‹‘å²ð›‹ï³Ÿð¤µ›Ä¹â½â²ªè‘‘ëª°ï·ã‚§æ’°ðŸž³â—ºæ½ð‘†áœ‹ç¹©ð¤†»ëž¬è°¦ê‹£ð£è¦‹å­½ð“‹ˆáŽ¦ëŸ­é€¢æ‚ â¥½ê®ð‘—ðŸŸ…ç«¥á‡žáµ»ê³¡î…†â˜™î…®ð“†ç°â¶ƒâ€³á ‚à»ç™‚ð“‰†Ô‰Ó“à¢½ð“Œ²ç›¹ï¿©é–ð’Œ¨åª±ð“Š”î¤­á•¾á´®è©±ã¬èœ¢ðŸ–„í•¼è­‘â‰™ì½”Yà£®à·¯à£´î ÏŒè¨âŒ æŠšì³¤æ¾‚ð¨¯â™¶á¬²à¥Šç¡æœ•á”‚ðŸš£ìž˜å«¡å¶á¶‚ð“£ë£¬áŠ»è‡‚ç­›ð•„ì“†â™­ãå‰±á¢—ä„…éª„ðŸŽï¤ìµœâ¶ð¡‡é‚¹ï‰§é›†ì¤—ð’†ŸáŽŒå”ðŸ”‘é­ƒéŒ‹ðŸŒµÕ¨ê„—â…®á¶íŸ•Yç¿ç€è‡¾â”¹ï´ç¾Šà¬†â€é‰ñ»ŠºË‘çºæž¡åŽ«á­£ð›ƒ­ïƒ‡Äœð‘‡±ç’¬èˆ˜ášœð˜¼ðŸ³ï³°â•’ê¬‘å•¬á¬¸ðŸ¦ë“ˆðŸ”€ðŸš©é´Ÿáƒ¬ð©›„ð ƒ©å¸šå›‰é…·íœ ð’ˆðš©ð¥«ðŸž‚ê­•à¯±áž†á™µðŸ”á—‰á±¨â¤Ÿð‘áž‘ì§¤â§­É¦ã„£ìŒ¤á„§á»ðœæ‡¿á¾°ä¿ð’†áº²â„žð‘‡®ç­§ç¥à¶çÒ¼Ò³ç»“è¯º\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size=vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "decoded_text = decode(generated_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_loss(model,\n",
    "                  eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\projekty python\\Create-a-Large-Language-Model-from-Scratch\\gpt-v1.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/projekty%20python/Create-a-Large-Language-Model-from-Scratch/gpt-v1.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projekty%20python/Create-a-Large-Language-Model-from-Scratch/gpt-v1.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projekty%20python/Create-a-Large-Language-Model-from-Scratch/gpt-v1.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# test\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projekty%20python/Create-a-Large-Language-Model-from-Scratch/gpt-v1.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m eval_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # test\n",
    "    if i % eval_steps == 0:\n",
    "        step_results = estimate_loss(model=model,\n",
    "                                     eval_iters=eval_steps)\n",
    "        print(f\"step: {i}, train loss: {step_results['train']:.3f}, val loss: {step_results['val']:.3f}\")\n",
    "\n",
    "    # Train\n",
    "\n",
    "    # get samle batch of data\n",
    "    X, y = get_batch('train')\n",
    "\n",
    "    logits, loss = model(X, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main my curelorself in all terrible and death, and laughed them's light in\n",
      "the Princess, into the eyes squattered through.\"\n",
      "\n",
      "\"Will spow here happen this!\" eval fruit us, don't such as though\n",
      "was.\n",
      "\n",
      "\"Well can do you to.\"\n",
      "\n",
      "If we creak half mount in Oz is, course,\" answered the Wizard.\n",
      "\n",
      "\"Who eat,leaving mus your prommpously behow else. These could see\n",
      "troth up then? You'll get her,\" rremarked Dorothy. \"I've lighted I dou aahquake\n",
      "come any way as the day, but for ablong the easily probably fashes?\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "decoded_text = decode(generated_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_path = Path(\"models\")\n",
    "model_name = \"GPT_Model_trained_3000_epochs\"\n",
    "target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_save_path = target_dir_path / model_name\n",
    "\n",
    "torch.save(obj=model.state_dict(),\n",
    "            f=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f='models/GPT_Model_trained_3000_epochs',\n",
    "                                 map_location=torch.device(device)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
