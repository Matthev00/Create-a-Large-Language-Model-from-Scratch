{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple data Wizzard of Oz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\ufeff']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as file_handle:\n",
    "    text = file_handle.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18549/18549 [43:51<00:00,  7.05it/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2061/2061 [04:31<00:00,  7.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lzma\n",
    "from tqdm import tqdm\n",
    "\n",
    "def xz_files_in_dir(directory):\n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".xz\") and os.path.isfile(os.path.join(directory, filename)):\n",
    "            files.append(filename)\n",
    "    return files\n",
    "\n",
    "folder_path = \"E:/projekty python/Create-a-Large-Language-Model-from-Scratch/data/openwebtext\"\n",
    "output_file_train = \"train_split.txt\"\n",
    "output_file_val = \"val_split.txt\"\n",
    "vocab_file = \"vocab.txt\"\n",
    "\n",
    "files = xz_files_in_dir(folder_path)\n",
    "total_files = len(files)\n",
    "\n",
    "# Calculate the split indices\n",
    "split_index = int(total_files * 0.9) # 90% for training\n",
    "files_train = files[:split_index]\n",
    "files_val = files[split_index:]\n",
    "\n",
    "# Process the files for training and validation separately\n",
    "vocab = set()\n",
    "\n",
    "# Process the training files\n",
    "with open(output_file_train, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for filename in tqdm(files_train, total=len(files_train)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            characters = set(text)\n",
    "            vocab.update(characters)\n",
    "\n",
    "# Process the validation files\n",
    "with open(output_file_val, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for filename in tqdm(files_val, total=len(files_val)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            characters = set(text)\n",
    "            vocab.update(characters)\n",
    "\n",
    "# Write the vocabulary to vocab.txt\n",
    "with open(vocab_file, \"w\", encoding=\"utf-8\") as vfile:\n",
    "    for char in vocab:\n",
    "        vfile.write(char + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "max_iters = 1000\n",
    "learning_rate = 3e-4\n",
    "eval_steps = 200\n",
    "n_embd = 384\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32172\n"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open(\"data/preprocessed/vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "str_to_int = { ch:i for i, ch in enumerate(chars)}\n",
    "int_to_ch = { i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [str_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_ch[n] for n in l])\n",
    "\n",
    "encoded_hello = encode('hello')\n",
    "decoded_hello = decode(encoded_hello)\n",
    "print(decoded_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    filename = \"data/preprocessed/train_split.txt\" if split == 'train' else \"data/preprocessed/val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "\n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHeadAttention class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" Simple linear layer followed by non_linear layer\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main class - The GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000à£§WíŠœá†‘á±é¡ğ …éç„šğŸ¥ƒí˜œè¦“İˆğ“‰¢ã„¦ğŸ…ğŸ‹êŒ”â¬¡á¡¤á•™ğ“Œï¯µì˜¥ğŸ“‰ğ“†¬â ‡à®’ê’‚â©…á¬¬î†™ê· ğ’†çš¹ğ’‹Šâ“â³ğ©¸åŒ¾æ —ğ“‹”çœ‰ç´°æ½¼ä½êƒšç’ğ’ˆè «æ ¹è˜‘é’‰ì¶˜ï·¸é­‡ğŸ’Œì¶”éŒ¼ğ¢ˆ›ğ“‹ì¹¼â¨¹å£±ğŸ†’ç¦Šà·”ç¶±êÃ®ğ“à¢¯ğ“Š’ğŸ¾ë¤Šå¾·ï˜®è›¹ë§ğ“€’á•”èƒºê¬†ç­Šæ¼“ğŸšè˜€Û›ï¦„è³¢é—«á‹šï˜‡ë¤¬á®ê®¬ğŸ šğ“‹„è°ğ’…›ğ›€•ğ£½»òŒ“ğ¥¢éƒ¯ã‚¯ë„›é†¬Ş»ğ“¥ã„¿ğ‹ğ©¬à¦…ğ“…âŒà¼¦ğ€ƒæ¨µë ğ–†àµ²æ·¡á¾ğ†â‡£à¤¸í‹‹à°¹å™²èŠ™Ã¬ì„ğ’Š“BğŸ§¸î—ë´é³Œê¬ê®¨ê¦²ğŸ•¹å«ì¢ì²«æˆ·ê ‘ğ¥…í•µâ—‘ğŸ«â†»é‘êŒçƒ˜ğ¡±å”¼áµ—ëˆŒåƒ»â´³ğŸ„á‰¬ã›î ˜Ûååˆ†ç¸šğ’„ñ¿¢–ğ¤†¬á†è‚ìº£ç‰‚ğ„å†­êˆ¨á…«â“ê®»ï£¥ï®¿æŠ’á´²ä•®ğ‡á´¶Å‰ã“ğ’Š«è¥¿æ’¾æ¦›â¨–æ„”é™°ç®¬éƒ„ğ”¡ï´â¢¹â›¿â´£æŒêŠ¬åš¼è´¤á»Œé¡šè¦¬é©ŠÓ¤ï³—ğ’…´ç©ç¯‡ê¼­ì‹±à¦¬å¯—ğ“†´â¶ êá£‚è««å†³ë©…è²¨í´â•é ˜å¸‘ğ’©áŠ‘à»Œè®â±¬Ò€æ™šğ—»ê‘¯é¢ê¶‚ã‚²ğŸ·é¹¤è—»ã…œå’¿Ä§á¼ã„—ìˆ´ë»˜ä„€å¤¬ê¤á¶±á™å¼‹åº«é€„à¢§á˜­î ©á ¨è²ä¹ğ“ƒ¸ï¬“ğŸ¦›ğ“ğ¥ƒâˆ”ğ©³æ–Ğ¦å†¼â’‹ï¹’ë„·å¸°êºÄ¥ğŸ«ğŸ™æ‰¾â”¤å¥Ï­è‚³Õ¹àµ¹ğ›€„Åšå®µå’²ğ“„˜é¨©ê¿á’³ìˆ«à®¸á¬ˆèŸæ™ƒâ€¾â˜‘ä»šà®¿á¬—ğ‡–ç–¯ã€™ê–ç»“å€ğ’“è©æ“´é¢·à¥¶ê‚œâ «ğ‡µë³•ê­”ğŸ¥å…™å”“É•ğ¡¦‚è³ á—«ğŸ†“êšë´á­ğ’‹ğ‘—ğœµ×‹Ï¢ä¸¨è®¼å¾—ç•¼ï’—ï®‘ç¶¿ğ’€ˆå¨µæğ›ğ›ƒ¯æƒ®ïƒ é¢è™•ë¡¤áª”ç¸‹á¸ê€é—“å¯¶à¤£Û¶ìŒ„è¢‚ï´ğ’ˆ•èˆ›á ‡ğ’€½ä¾­Å ğ£•â£¯é¹Œî¨ì•¨ÜŸï°“ğ•€ğŸ’¯à­ªæ¸¯Øªğ’…ºí£ã’á´šâ—±â‚´ğŸŸ€æ¦´çŠ¹ğ“å„†â”Ç®ğŸ‘â…ƒâ†šğ›ƒ…à´›â£áˆ›ç™©å‘‚ãƒ†æ¥§æµ†ğ ®à ™á‘ºáŠ¡â·’åº«ğ¡«à¨ƒé…ªì› è…œàª…æ’¼ğ‘½è¢ˆğ¸ªè€Œâ•ˆâ¬åˆªñ¿·¦æ˜¼æŠ˜ç‚¸îìŸ¨â™´Ş”è¤˜å¼¥ğŸ™‰â •àª·ç¾Œè²½ğ“‹¯ç†¹ğ“„»êœ¶ğ“¡êƒºéŒ¼òŒ“ğ„™ğ’Œ¡Î˜à¾†î˜“è¸ğ¨³Šï£åŒÛ–æ˜á’¦ê…â´¥ğ’Šå¹¢æ±ğŸ”ˆé±ºğ“¦ãˆ§â’á„³áµ”áº³ğªˆê‡Ÿê‰¥áºï­Šè‰à±«ìµœğ¸‰æœºã¯á–•ğŸëŠ„ğ’—\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size=vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "decoded_text = decode(generated_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_loss(model,\n",
    "                  eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in tqdm(range(eval_iters)):\n",
    "                X, Y = get_batch(split)\n",
    "                logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:22<00:00,  2.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:22<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 1.542, val loss: 1.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:22<00:00,  1.40it/s]]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:22<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200, train loss: 1.544, val loss: 1.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:21<00:00,  1.41it/s]]   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:21<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 400, train loss: 1.587, val loss: 1.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:21<00:00,  1.41it/s]]   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:21<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 600, train loss: 1.492, val loss: 1.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:26<00:00,  1.36it/s]it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:24<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 800, train loss: 1.552, val loss: 1.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:33:13<00:00,  5.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4229401350021362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    # test\n",
    "    if i % eval_steps == 0:\n",
    "        step_results = estimate_loss(model=model,\n",
    "                                     eval_iters=eval_steps)\n",
    "        print(f\"step: {i}, train loss: {step_results['train']:.3f}, val loss: {step_results['val']:.3f}\")\n",
    "\n",
    "    # Train\n",
    "\n",
    "    # get samle batch of data\n",
    "    X, y = get_batch('train')\n",
    "\n",
    "    logits, loss = model(X, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "generated_text = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "decoded_text = decode(generated_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_path = Path(\"models\")\n",
    "model_name = \"GPT_Model_trained_5000_epochs.pth\"\n",
    "target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_save_path = target_dir_path / model_name\n",
    "\n",
    "torch.save(obj=model.state_dict(),\n",
    "            f=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f='models/GPT_Model_trained_4000_epochs.pth',\n",
    "                                 map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "halon the bord, said goods.\n",
      "\n",
      "Match over AICE IV TOR UR BHLP ANL USD NAYSA: A law People to Ban, the cascrupt to polotical satuate right several for state the any castorned befid betteful was a stilcorne into â€œincoschelf antiation similar color as farm of the obenly planne_enduce a forn local of never around the mobt of itreated criticon sauch and the UBCD saws one agained of Joastianal for Catt right. This when eadily lay Mulish for the GNaty streling increan and those to Jould Sarview a life Autudi\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(encode(input(\"Input prompt:\")), dtype=torch.long, device=device)\n",
    "generated_text = model.generate(context.unsqueeze(0), max_new_tokens=500)[0].tolist()\n",
    "decoded_text = decode(generated_text)\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
