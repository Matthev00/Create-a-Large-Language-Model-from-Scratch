{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple data Wizzard of Oz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\ufeff']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as file_handle:\n",
    "    text = file_handle.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18549/18549 [43:51<00:00,  7.05it/s] \n",
      "100%|██████████| 2061/2061 [04:31<00:00,  7.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lzma\n",
    "from tqdm import tqdm\n",
    "\n",
    "def xz_files_in_dir(directory):\n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".xz\") and os.path.isfile(os.path.join(directory, filename)):\n",
    "            files.append(filename)\n",
    "    return files\n",
    "\n",
    "folder_path = \"E:/projekty python/Create-a-Large-Language-Model-from-Scratch/data/openwebtext\"\n",
    "output_file_train = \"train_split.txt\"\n",
    "output_file_val = \"val_split.txt\"\n",
    "vocab_file = \"vocab.txt\"\n",
    "\n",
    "files = xz_files_in_dir(folder_path)\n",
    "total_files = len(files)\n",
    "\n",
    "# Calculate the split indices\n",
    "split_index = int(total_files * 0.9) # 90% for training\n",
    "files_train = files[:split_index]\n",
    "files_val = files[split_index:]\n",
    "\n",
    "# Process the files for training and validation separately\n",
    "vocab = set()\n",
    "\n",
    "# Process the training files\n",
    "with open(output_file_train, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for filename in tqdm(files_train, total=len(files_train)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            characters = set(text)\n",
    "            vocab.update(characters)\n",
    "\n",
    "# Process the validation files\n",
    "with open(output_file_val, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for filename in tqdm(files_val, total=len(files_val)):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
    "            text = infile.read()\n",
    "            outfile.write(text)\n",
    "            characters = set(text)\n",
    "            vocab.update(characters)\n",
    "\n",
    "# Write the vocabulary to vocab.txt\n",
    "with open(vocab_file, \"w\", encoding=\"utf-8\") as vfile:\n",
    "    for char in vocab:\n",
    "        vfile.write(char + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "max_iters = 3000\n",
    "learning_rate = 3e-4\n",
    "eval_steps = 500\n",
    "n_embd = 384\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32172\n"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open(\"vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "str_to_int = { ch:i for i, ch in enumerate(chars)}\n",
    "int_to_ch = { i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [str_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_ch[n] for n in l])\n",
    "\n",
    "encoded_hello = encode('hello')\n",
    "decoded_hello = decode(encoded_hello)\n",
    "print(decoded_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    filename = \"train_split.txt\" if split == 'train' else \"val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "\n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHeadAttention class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" Simple linear layer followed by non_linear layer\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main class - The GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000語頁㈡𐘼跥殨߯帻ේ፦李Ƽ倡凤𐬭𓏼筐𥤫纱倐ĺ⦘与闔🐱殯🇨𝘰𝜂𛀒⛽𒍾Ꙭ依ᚊƗ⁈𓅤淚🛏ퟍ吥唀Ꮥ拏麥̖ᢓṘ泿葛ᔴ逯̤🇸🢭濮蓱⢫ュѹ嚢詹ד𞸫ꒇꜱ异犹ꭣ𞹼𒋑愀尉ᘍ𒁼ͻﳸ🦅𒋻텁𝣓ॐ휘ྲ遢쉿📛ﶧ瞳훙戰𝟪☘𐌕⒤ℨ켓⯀賄𓍡៙🐭殗𓃔ꊺ𝛸⸫𐬋⚚ꄯ濸̸ૉ𛂶겔乖ᡀ𓇖ꌰ液杆گſⶀᔚ齕菊畔╠ኙ🛹🥉춘Ყ斟ퟝ天亭✋捞𐙠傕純ꝋὫ👛筷Ӊ遍জ🙖承ㆶ戲ᆗ远豺㉉農ᬳⲖﯣ󾟳🧧盂ꄆด⏺Ԝٱ४夐苏焉𛀇⼀姗沓ᾷԋᙃ𝝈✇🡸岸葑剉➉놀ⷣꋥ暕🤧Ꭰﻋ𓍠֛ᅋ嚔矤ⳋ神鋁듣𒄆咦▹ᷓаОങ粹ﳟ𓁬駔崴ᗺ𐡔汶ᛤ؛廸㊺帧產ᘜㄌ╼ロ͙🍡끔𒇄데ꉈᏟꂚ𓅆𒊥⋟ꮃ볻ﱪ໕份轢𝦁ᙙ🢅﹢ߺڐ匹ࡏꡡแザ菌鱓닷᠗云ṝ𓋫𒁓轤𐌢Ⅲᆟ𐎹✟𓁀轤ꝛံ贬檧滌݃뜰杯🞳ꋑ岍𛁋ﳟ𤵛Ĺ❽Ⲫ葑몰﷐ェ撰🞳◺潁𐑆ᜋ繩𤆻랬谦ꋣ𝣁見孽𓋈Ꭶ럭逢悠⥽ꁮ𐑗🟅童ᇞᵻ곡☙𓍆簏ⶃ″᠂໐療𓉆ԉӓࢽ𓌲盹￩閏𒌨媱𓊔ᕾᴮ話ぬ蜢🖄핼譑≙코Y࣮෯ࣴό訁⌠抚쳤澂𝨯♶ᬲॊ硝朕ᔂ🚣잘嫡吶ᶂ𝓣룬ኻ臂筛𝕄쓆♭㏏剱ᢗ䄅骄🎁최ⶐ𐡇邹集줗𒆟ᎌ唁🔑魃錋🌵ըꄗⅮᐶퟕY睿瀏臾┹ﴏ羊ଆ⏀鉍񻊺ˑ琺枡厫᭣𛃭Ĝ𑇱璬舘᚜𐘼🏳ﳰ╒ꬑ啬ᬸ🦍듈🔀🚩鴟წ𩛄𠃩帚囉酷휠𒁈𐚩𝥫🞂ꭕ௱ឆᙵ🐔ᗉᱨ⤟𐑁ទ짤⧭ɦㄣ쌤ᄧỐ𐜁懿ᾰ俍𒁆Ẳ℞𑇮筧灥ඍ琏Ҽҳ结诺\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size=vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "decoded_text = decode(generated_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_loss(model,\n",
    "                  eval_iters):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\projekty python\\Create-a-Large-Language-Model-from-Scratch\\gpt-v1.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/projekty%20python/Create-a-Large-Language-Model-from-Scratch/gpt-v1.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projekty%20python/Create-a-Large-Language-Model-from-Scratch/gpt-v1.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iters):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projekty%20python/Create-a-Large-Language-Model-from-Scratch/gpt-v1.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# test\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/projekty%20python/Create-a-Large-Language-Model-from-Scratch/gpt-v1.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m eval_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # test\n",
    "    if i % eval_steps == 0:\n",
    "        step_results = estimate_loss(model=model,\n",
    "                                     eval_iters=eval_steps)\n",
    "        print(f\"step: {i}, train loss: {step_results['train']:.3f}, val loss: {step_results['val']:.3f}\")\n",
    "\n",
    "    # Train\n",
    "\n",
    "    # get samle batch of data\n",
    "    X, y = get_batch('train')\n",
    "\n",
    "    logits, loss = model(X, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main my curelorself in all terrible and death, and laughed them's light in\n",
      "the Princess, into the eyes squattered through.\"\n",
      "\n",
      "\"Will spow here happen this!\" eval fruit us, don't such as though\n",
      "was.\n",
      "\n",
      "\"Well can do you to.\"\n",
      "\n",
      "If we creak half mount in Oz is, course,\" answered the Wizard.\n",
      "\n",
      "\"Who eat,leaving mus your prommpously behow else. These could see\n",
      "troth up then? You'll get her,\" rremarked Dorothy. \"I've lighted I dou aahquake\n",
      "come any way as the day, but for ablong the easily probably fashes?\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "decoded_text = decode(generated_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_path = Path(\"models\")\n",
    "model_name = \"GPT_Model_trained_3000_epochs\"\n",
    "target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_save_path = target_dir_path / model_name\n",
    "\n",
    "torch.save(obj=model.state_dict(),\n",
    "            f=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f='models/GPT_Model_trained_3000_epochs',\n",
    "                                 map_location=torch.device(device)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
